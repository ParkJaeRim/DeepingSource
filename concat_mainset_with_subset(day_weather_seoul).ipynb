{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"./dataset/Traffic accident info/Traffic accident info(12.1 ~ 17.6).csv\", encoding='ms949')\n",
    "day_weather = pd.read_csv(\"./dataset/subdata/11. Observe daily weather in several district/observe daily weather(2012~2017).csv\", encoding='ms949')\n",
    "\n",
    "# '발생년', '사고유형', '법규위반_대분류', '도로형태', '당사자종별_1당', '당사자종별_2당'\n",
    "for idx in [22,20,18,15,14, 0]:\n",
    "    train = train.drop(columns=train.columns[idx])\n",
    "    \n",
    "train_col = train.columns\n",
    "day_weather_col = day_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2050 2050\n"
     ]
    }
   ],
   "source": [
    "# Get 발생지시도 - 서울 dataset \n",
    "train_array = np.array(train)\n",
    "seoul_dataset = []\n",
    "\n",
    "for i in range(len(train_array)):\n",
    "    if train_array[i,9] == '서울':\n",
    "        seoul_dataset.append(train_array[i])\n",
    "        \n",
    "seoul_dataset = np.array(seoul_dataset)\n",
    "\n",
    "day_order = seoul_dataset[:,0]\n",
    "sorted_order = sorted(range(len(day_order)), key=lambda k: day_order[k])\n",
    "seoul_dataset=seoul_dataset[[sorted_order]]\n",
    "\n",
    "# check result\n",
    "print(len(seoul_dataset),list(train['발생지시도']).count('서울'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make day_weather DataFrame to dict\n",
    "uniq_days = list(map(str,set(day_weather[day_weather.columns[0]])))\n",
    "uniq_regions = list(set(day_weather[day_weather.columns[1]]))\n",
    "\n",
    "day_weather_dict = dict.fromkeys(sorted(uniq_days))\n",
    "\n",
    "for day in uniq_days:\n",
    "    day_weather_dict[day] = dict.fromkeys(sorted(uniq_regions))\n",
    "    for rg in uniq_regions:\n",
    "        day_weather_dict[day][rg] = list()\n",
    "\n",
    "for day in np.array(day_weather):\n",
    "    day_weather_dict[str(day[0])][day[1]].append(day[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat Seoul Traffic Dataset with Seoul Day Weather\n",
    "seoul_dataset = list(seoul_dataset)\n",
    "error_indices = []\n",
    "\n",
    "# try concat but except when missing data or key values error has been raised\"\n",
    "for i in range(len(seoul_dataset)):\n",
    "    rg = '중구' if seoul_dataset[i][10] == '중구' else seoul_dataset[i][10][:-1]\n",
    "    try :\n",
    "        if not day_weather_dict[str(seoul_dataset[i][0])[:8]][rg] :\n",
    "            error_indices.append(i)\n",
    "        else :\n",
    "            seoul_dataset[i] = np.concatenate((seoul_dataset[i],\n",
    "                                               day_weather_dict[str(seoul_dataset[i][0])[:8]][rg][0]))\n",
    "    except : \n",
    "        error_indices.append(i)\n",
    "\n",
    "# Remove Error rows by missing data\n",
    "for i in list(reversed(error_indices)):\n",
    "    seoul_dataset.pop(i)\n",
    "    \n",
    "seoul_dataset = np.array(seoul_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make processed Dataset to csv file without one-hot encoding\n",
    "seoul_col = train_col.append(day_weather_col[2:])\n",
    "seoul_dataset_pd = pd.DataFrame(seoul_dataset, columns=seoul_col)\n",
    "seoul_dataset_pd.to_csv(\"./concat_ver1.csv\",index=None,encoding='ms949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns' names that have string values For one-hot encoding\n",
    "str_col = []\n",
    "for i, w in enumerate(seoul_dataset[0]):\n",
    "    if type(w) is str:\n",
    "        str_col.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get\n",
    "uniq_class = []\n",
    "\n",
    "for idx in str_col:\n",
    "    uniq_class.append(list(sorted(set(seoul_dataset[:,idx]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(seoul_dataset)):\n",
    "    for i, col in enumerate(str_col):\n",
    "        seoul_dataset[idx,col] = int(uniq_class[i].index(seoul_dataset[idx,col]))\n",
    "        \n",
    "seoul_dataset_pd = pd.DataFrame(seoul_dataset, columns=seoul_col)\n",
    "seoul_dataset_pd.to_csv(\"./concat_ver2.csv\",index=None,encoding='ms949')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
